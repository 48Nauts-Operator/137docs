<!-- Auto-generated by Claude on 2025-06-02 06:15 -->

# babel.messages.jslexer

A JavaScript 1.5 lexer module used for extracting translatable strings from JavaScript code in the Babel internationalization toolkit.

## Purpose

This module provides lexical analysis (tokenization) capabilities for JavaScript source code. It's specifically designed to support the JavaScript message extractor in Babel, which needs to parse JavaScript files to identify translatable strings and function calls.

## Key Features

- **JavaScript tokenization**: Breaks JavaScript source code into meaningful tokens
- **JSX support**: Limited parsing support for JSX syntax
- **ES6 template strings**: Support for backtick-delimited template literals
- **String unquoting**: Proper handling of JavaScript string escape sequences
- **Flexible configuration**: Configurable parsing options for different JavaScript variants

## Classes

### `Token`

A named tuple representing a single token in the JavaScript source.

```python
class Token(NamedTuple):
    type: str      # Token type (e.g., 'string', 'name', 'operator')
    value: str     # The actual token value from source
    lineno: int    # Line number where token appears
```

## Key Functions

### `tokenize(source, jsx=True, dotted=True, template_string=True, lineno=1)`

Main tokenization function that processes JavaScript source code.

**Parameters:**
- `source` (str): The JavaScript source code to tokenize
- `jsx` (bool): Enable limited JSX parsing (default: True)
- `dotted` (bool): Treat dotted names as single tokens (default: True)
- `template_string` (bool): Support ES6 template strings (default: True)
- `lineno` (int): Starting line number (default: 1)

**Returns:** Generator yielding `Token` objects

**Example:**
```python
source = 'var message = "Hello, world!";'
for token in tokenize(source):
    print(f"{token.type}: {token.value}")
```

### `unquote_string(string)`

Processes JavaScript string literals by removing quotes and handling escape sequences.

**Parameters:**
- `string` (str): A quoted string literal (must start/end with matching quotes)

**Returns:** Unquoted string with escape sequences processed

**Example:**
```python
quoted = '"Hello\\nWorld"'
unquoted = unquote_string(quoted)  # Returns: "Hello\nWorld"
```

### `indicates_division(token)`

Helper function to determine if a token can be followed by a division operator. This helps distinguish between division operators (`/`) and regex literals (`/pattern/`).

**Parameters:**
- `token` (Token): The token to analyze

**Returns:** Boolean indicating if division operator is expected

## Token Types

The lexer recognizes these token types:

- **`name`**: Identifiers and variable names
- **`dotted_name`**: Dot-separated identifiers (when enabled)
- **`number`**: Numeric literals (integers, floats, hex)
- **`string`**: String literals (single/double quoted)
- **`template_string`**: ES6 template literals (backtick-quoted)
- **`operator`**: JavaScript operators and punctuation
- **`regexp`**: Regular expression literals
- **`jsx_tag`**: JSX tag elements (when enabled)
- **`linecomment`**: Single-line comments (`//`)
- **`multilinecomment`**: Multi-line comments (`/* */`)

## Regular Expressions

The module uses several compiled regex patterns for efficient tokenization:

- `name_re`: Matches JavaScript identifiers
- `dotted_name_re`: Matches dot-separated identifiers
- `division_re`: Matches division operators
- `regex_re`: Matches regex literals
- `line_re`: Matches line endings

## Implementation Notes

### Context-Sensitive Parsing

The lexer uses context-sensitive parsing to handle the ambiguity between division operators and regex literals:

```javascript
// Division operator context
var result = a / b;

// Regex literal context  
var pattern = /[a-z]+/i;
```

### Escape Sequence Handling

The `unquote_string` function properly handles JavaScript escape sequences:

- Standard escapes: `\n`, `\t`, `\r`, etc.
- Unicode escapes: `\uXXXX`
- Hex escapes: `\xXX`
- Line continuation: `\` followed by newline

### Performance Considerations

- Operators are sorted by length (longest first) for efficient matching
- Compiled regex patterns avoid repeated compilation
- Generator-based tokenization for memory efficiency

## Usage Recommendations

1. **For message extraction**: Use default settings with all features enabled
2. **For strict JavaScript**: Disable JSX if not needed
3. **For older JavaScript**: Disable template strings for pre-ES6 code
4. **For performance**: Consider disabling unused features

## Error Handling

The lexer is designed to be tolerant of syntax errors:
- Invalid characters are skipped (advances position by 1)
- Malformed tokens don't crash the parser
- Line numbers are tracked even through invalid syntax

This makes it suitable for extracting strings from potentially incomplete or syntactically incorrect JavaScript files during development.