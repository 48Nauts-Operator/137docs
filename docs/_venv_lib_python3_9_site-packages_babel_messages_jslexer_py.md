<!-- Auto-generated by Claude on 2025-06-08 06:13 -->

# babel.messages.jslexer

## Overview

This module provides a JavaScript 1.5 lexer specifically designed for the Babel JavaScript extractor. It tokenizes JavaScript and JSX source code into a stream of tokens that can be used for further processing, such as extracting translatable strings from JavaScript files.

## Purpose

The lexer serves as a fundamental component in Babel's message extraction pipeline, allowing the system to:

- Parse JavaScript and JSX syntax
- Identify different types of tokens (strings, operators, names, etc.)
- Handle JavaScript-specific features like template strings and regular expressions
- Support both standard JavaScript and JSX syntax

## Key Components

### Token Class

```python
class Token(NamedTuple):
    type: str
    value: str
    lineno: int
```

A named tuple representing a single token with:
- **type**: The category of token (e.g., 'string', 'operator', 'name')
- **value**: The actual text content of the token
- **lineno**: The line number where the token appears

### Global Variables

#### Operators
```python
operators: list[str]
```
A comprehensive list of JavaScript operators sorted by length (longest first) to ensure proper tokenization precedence.

#### Regular Expressions
- **`name_re`**: Matches JavaScript identifiers and variable names
- **`dotted_name_re`**: Matches dotted property access patterns
- **`division_re`**: Matches division operators (`/` and `/=`)
- **`regex_re`**: Matches regular expression literals
- **`line_re`**: Matches different line ending patterns

#### Escape Sequences
```python
escapes: dict[str, str]
```
Maps JavaScript escape sequences to their actual character values.

## Important Functions

### `tokenize(source, jsx=True, dotted=True, template_string=True, lineno=1)`

**Main tokenization function** that converts JavaScript source code into a generator of tokens.

**Parameters:**
- `source`: The JavaScript source code to tokenize
- `jsx`: Enable JSX parsing support (default: True)
- `dotted`: Treat dotted names as single tokens (default: True)  
- `template_string`: Support ES6 template strings (default: True)
- `lineno`: Starting line number (default: 1)

**Returns:** Generator yielding `Token` objects

### `unquote_string(string)`

Processes JavaScript string literals by:
- Removing quote delimiters
- Handling escape sequences (including Unicode and hex escapes)
- Processing line continuations
- Converting escaped characters to their actual values

**Important:** The input string must be properly delimited with matching quotes.

### `indicates_division(token)`

**Context-sensitive parsing helper** that determines whether a token can be followed by a division operator. This is crucial for distinguishing between:
- Division operators (`/`, `/=`)
- Regular expression literals (`/pattern/flags`)

### `get_rules(jsx, dotted, template_string)`

Internal function that builds the tokenization rules based on syntax options.

## Usage Example

```python
from babel.messages.jslexer import tokenize

source = '''
const message = "Hello, world!";
const element = <div>{message}</div>;
'''

for token in tokenize(source, jsx=True):
    print(f"{token.type}: {token.value} (line {token.lineno})")
```

## Token Types

The lexer recognizes the following token types:

- **`string`**: String literals (`"hello"`, `'world'`)
- **`template_string`**: ES6 template strings (`` `hello ${name}` ``)
- **`name`**: Identifiers and variable names
- **`dotted_name`**: Property access chains (`obj.prop.method`)
- **`number`**: Numeric literals (integers, floats, hex)
- **`operator`**: JavaScript operators and punctuation
- **`regexp`**: Regular expression literals
- **`jsx_tag`**: JSX tags and fragments
- **`linecomment`**: Single-line comments (`//`)
- **`multilinecomment`**: Multi-line comments (`/* */`)

## Notes and Limitations

### Context-Sensitive Parsing
The lexer implements context-sensitive parsing to handle the ambiguity between division operators and regular expressions, which is a known challenge in JavaScript parsing.

### JSX Support
JSX support is somewhat limited and may not handle all edge cases of complex JSX syntax.

### Error Handling
When invalid syntax is encountered, the lexer skips one character and continues, which may not be suitable for strict parsing requirements.

### Performance Considerations
The lexer processes rules in order, with longer operators matched first to avoid tokenization conflicts.

## Dependencies

- `re`: Python regular expression module
- `collections.abc.Generator`: For type hints
- `typing.NamedTuple`: For the Token class definition

This lexer is specifically designed for Babel's message extraction needs and may not be suitable as a general-purpose JavaScript parser.