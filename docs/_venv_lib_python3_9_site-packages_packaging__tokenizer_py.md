<!-- Auto-generated by Claude on 2025-06-08 06:13 -->

# Tokenizer Module Documentation

## Overview

This module provides a lexical analyzer (tokenizer) for parsing Python package specification strings. It implements a context-sensitive token parser that can break down complex package requirement strings into manageable tokens for further processing.

## Purpose

The tokenizer is designed to parse Python package specifications that include:
- Package names and identifiers
- Version specifiers
- Environment markers (platform, Python version, etc.)
- URLs and extras
- Logical operators and grouping symbols

## Classes

### `Token`

A data class representing a single token in the input stream.

```python
@dataclass
class Token:
    name: str      # Token type name
    text: str      # Actual text content
    position: int  # Position in source string
```

**Fields:**
- `name`: The type of token (e.g., "IDENTIFIER", "VERSION", "COMMA")
- `text`: The actual text that was matched
- `position`: Starting position in the source string

### `ParserSyntaxError`

Custom exception class for syntax errors during parsing.

```python
class ParserSyntaxError(Exception):
    def __init__(self, message: str, *, source: str, span: tuple[int, int]) -> None:
```

**Features:**
- Provides detailed error messages with source context
- Shows error location with visual markers
- Includes span information for precise error positioning

**Example output:**
```
Expected closing parenthesis
python_version >= "3.8" and (os_name == "posix"
                              ~~~~~~~~~~~~~~~~~^
```

### `Tokenizer`

The main tokenizer class that performs context-sensitive token parsing.

```python
class Tokenizer:
    def __init__(self, source: str, *, rules: dict[str, str | re.Pattern[str]]) -> None:
```

#### Key Methods

##### `check(name: str, *, peek: bool = False) -> bool`
- Checks if the next token matches the given name
- If `peek=True`, doesn't load the token for reading
- Returns `True` if token matches, `False` otherwise

##### `consume(name: str) -> None`
- Moves past the specified token if it exists at current position
- Useful for optional tokens

##### `expect(name: str, *, expected: str) -> Token`
- Expects a specific token type, raises `ParserSyntaxError` if not found
- Returns the token without consuming it
- Used for mandatory tokens

##### `read() -> Token`
- Consumes and returns the next token
- Must be called after a successful `check()` or `expect()`

##### `enclosing_tokens(open_token: str, close_token: str, *, around: str)`
- Context manager for handling paired tokens (parentheses, brackets)
- Automatically handles opening and closing tokens
- Provides better error messages for mismatched pairs

## Token Rules

The `DEFAULT_RULES` dictionary defines the regular expressions for different token types:

### Structural Tokens
- `LEFT_PARENTHESIS`, `RIGHT_PARENTHESIS`: `(` and `)`
- `LEFT_BRACKET`, `RIGHT_BRACKET`: `[` and `]`
- `SEMICOLON`, `COMMA`: `;` and `,`

### Operators
- `OP`: Comparison operators (`===`, `==`, `~=`, `!=`, `<=`, `>=`, `<`, `>`)
- `BOOLOP`: Boolean operators (`or`, `and`)
- `IN`, `NOT`: Keyword operators

### Identifiers and Values
- `IDENTIFIER`: Package names and general identifiers
- `QUOTED_STRING`: Single or double-quoted strings
- `VARIABLE`: Environment variables (e.g., `python_version`, `os_name`)
- `SPECIFIER`: Version specifiers
- `URL`: URLs for package locations

### Version-related
- `VERSION_PREFIX_TRAIL`: Wildcard version suffixes (`.*`)
- `VERSION_LOCAL_LABEL_TRAIL`: Local version identifiers (`+dev1`)

## Usage Example

```python
# Initialize tokenizer
tokenizer = Tokenizer("requests >= 2.0.0", rules=DEFAULT_RULES)

# Check for identifier
if tokenizer.check("IDENTIFIER"):
    package_name = tokenizer.read()
    print(f"Package: {package_name.text}")

# Expect version specifier
version_spec = tokenizer.expect("SPECIFIER", expected="version specifier")
print(f"Version: {version_spec.text}")
```

## Notes and Suggestions

### Best Practices
- Always call `read()` after a successful `check()` before performing another `check()`
- Use `expect()` for mandatory tokens and `check()` + `consume()` for optional ones
- Leverage the `enclosing_tokens()` context manager for paired tokens

### Error Handling
- The tokenizer provides detailed error messages with source context
- Syntax errors include visual markers showing exact error locations
- Use custom error messages in `expect()` calls for better user experience

### Performance Considerations
- Regular expressions are compiled once during initialization
- The tokenizer processes input character by character, suitable for typical package specification lengths
- Consider caching tokenizer instances for repeated parsing of similar strings

### Extensibility
- Token rules can be customized by providing a different `rules` dictionary
- New token types can be added by extending the rules
- The tokenizer is designed to be context-sensitive and can handle complex parsing scenarios