<!-- Auto-generated by Claude on 2025-06-02 06:15 -->

# Tokenizer Module Documentation

## Overview

This module provides a comprehensive tokenization system for parsing Python package specification strings and dependency requirements. It implements a context-sensitive tokenizer with custom error handling and a flexible rule-based token matching system.

## Purpose

The tokenizer is designed to parse complex dependency specifications that include:
- Package names and identifiers
- Version specifiers and constraints
- Environment markers and variables
- URL specifications
- Logical operators for conditional dependencies

## Classes

### `Token`

A simple dataclass representing a parsed token.

```python
@dataclass
class Token:
    name: str      # Token type identifier
    text: str      # Actual text content
    position: int  # Position in source string
```

**Fields:**
- `name`: The token type (e.g., "IDENTIFIER", "VERSION", "COMMA")
- `text`: The raw text that was matched
- `position`: Starting position in the original source string

### `ParserSyntaxError`

Custom exception class for parsing errors with enhanced error reporting.

```python
class ParserSyntaxError(Exception):
    def __init__(self, message: str, *, source: str, span: tuple[int, int]) -> None:
```

**Features:**
- Stores error location span within the source text
- Provides formatted error messages with visual indicators
- Shows the problematic source line with position markers

**Example output:**
```
Expected closing parenthesis
package>=1.0 (extra==test
             ~~~~~~~~~^
```

### `Tokenizer`

The main tokenization engine that provides context-sensitive token parsing.

```python
class Tokenizer:
    def __init__(self, source: str, *, rules: dict[str, str | re.Pattern[str]]) -> None:
```

#### Key Methods

##### `check(name: str, *, peek: bool = False) -> bool`
Checks if the next token matches the specified type without consuming it.
- `peek=True`: Performs check without loading the token (allows multiple checks)
- `peek=False`: Loads the token, requiring `read()` before next check

##### `consume(name: str) -> None`
Convenience method that checks for a token and reads it if present.

##### `expect(name: str, *, expected: str) -> Token`
Expects a specific token type, raising `ParserSyntaxError` if not found.

##### `read() -> Token`
Consumes and returns the currently loaded token.

##### `enclosing_tokens(open_token: str, close_token: str, *, around: str)`
Context manager for handling paired tokens (parentheses, brackets, etc.).

```python
with tokenizer.enclosing_tokens("LEFT_PARENTHESIS", "RIGHT_PARENTHESIS", around="extras"):
    # Parse content between parentheses
    pass
```

## Token Rules

The `DEFAULT_RULES` dictionary defines the regex patterns for token recognition:

### Structural Tokens
- `LEFT_PARENTHESIS`, `RIGHT_PARENTHESIS`: `(` and `)`
- `LEFT_BRACKET`, `RIGHT_BRACKET`: `[` and `]`
- `SEMICOLON`, `COMMA`: `;` and `,`

### Operators
- `OP`: Comparison operators (`===`, `==`, `~=`, `!=`, `<=`, `>=`, `<`, `>`)
- `BOOLOP`: Boolean operators (`or`, `and`)
- `IN`, `NOT`: Keyword operators

### Environment Variables
- `VARIABLE`: Python environment markers like:
  - `python_version`, `python_full_version`
  - `os_name`, `sys_platform`
  - `platform_release`, `platform_system`
  - `extras`, `dependency_groups`

### Identifiers and Values
- `IDENTIFIER`: Package names and general identifiers
- `QUOTED_STRING`: Single or double-quoted strings
- `SPECIFIER`: Version specifiers (integrates with `Specifier` class)
- `URL`: URL specifications for direct package links

## Usage Example

```python
# Initialize tokenizer with source text
tokenizer = Tokenizer("requests>=2.0.0", rules=DEFAULT_RULES)

# Check and consume tokens
if tokenizer.check("IDENTIFIER"):
    package_token = tokenizer.read()
    print(f"Package: {package_token.text}")

if tokenizer.check("SPECIFIER"):
    version_token = tokenizer.read()
    print(f"Version: {version_token.text}")
```

## Notes and Suggestions

### Best Practices
- Always use `expect()` when a token is required for valid syntax
- Use `enclosing_tokens()` context manager for paired delimiters
- Handle `ParserSyntaxError` exceptions to provide user-friendly error messages

### Performance Considerations
- Regex patterns are compiled once during initialization
- Token matching is performed lazily (only when checked)
- Consider the order of token rules for optimal matching

### Extension Points
- Custom token rules can be provided during initialization
- The rule system supports both string patterns and compiled regex objects
- Token types can be extended for domain-specific parsing needs

### Potential Improvements
- Add support for custom error recovery strategies
- Implement token lookahead beyond single token
- Consider adding line/column position tracking for better error reporting