<!-- Auto-generated by Claude on 2025-06-08 06:13 -->

# Parser Module Documentation

This module provides tokenization and parsing utilities for processing structured text, particularly focused on package specification syntax. It includes token representation, error handling, and a context-sensitive tokenizer.

## Overview

The module implements a lexical analyzer (tokenizer) that can break down source text into meaningful tokens according to predefined rules. It's designed to parse package specifications, version constraints, and dependency declarations commonly found in Python packaging.

## Classes

### `Token`

A simple dataclass representing a parsed token.

```python
@dataclass
class Token:
    name: str      # Token type/category
    text: str      # Actual text content
    position: int  # Position in source text
```

**Fields:**
- `name`: The token type (e.g., "IDENTIFIER", "SPECIFIER")
- `text`: The actual text content that was matched
- `position`: Starting position in the source string

### `ParserSyntaxError`

Custom exception class for parsing errors with enhanced error reporting.

```python
class ParserSyntaxError(Exception):
    def __init__(self, message: str, *, source: str, span: tuple[int, int]) -> None:
```

**Features:**
- Provides detailed error messages with source context
- Shows error location with visual markers
- Includes span information for error highlighting

**Example output:**
```
Expected closing parenthesis
some_package(>=1.0
            ~~~~~~^
```

### `Tokenizer`

The main tokenization engine that provides context-sensitive token parsing.

```python
class Tokenizer:
    def __init__(self, source: str, *, rules: dict[str, str | re.Pattern[str]]) -> None:
```

## Key Methods

### `check(name: str, *, peek: bool = False) -> bool`

Checks if the next token matches the specified name without consuming it.

- **Parameters:**
  - `name`: Token name to check for
  - `peek`: If True, doesn't load the token for reading
- **Returns:** Boolean indicating if token matches

### `consume(name: str) -> None`

Convenience method to consume a token if it matches the specified name.

### `expect(name: str, *, expected: str) -> Token`

Expects a specific token type, raising a syntax error if not found.

- **Parameters:**
  - `name`: Expected token name
  - `expected`: Human-readable description for error messages

### `read() -> Token`

Consumes and returns the next token that was previously checked.

### `enclosing_tokens(open_token: str, close_token: str, *, around: str)`

Context manager for handling paired tokens (like parentheses or brackets).

```python
with tokenizer.enclosing_tokens("LEFT_PARENTHESIS", "RIGHT_PARENTHESIS", around="version spec"):
    # Parse content between parentheses
    pass
```

## Token Rules

The `DEFAULT_RULES` dictionary defines regex patterns for various token types:

### Structural Tokens
- `LEFT_PARENTHESIS`, `RIGHT_PARENTHESIS`: `(` and `)`
- `LEFT_BRACKET`, `RIGHT_BRACKET`: `[` and `]`
- `SEMICOLON`, `COMMA`: `;` and `,`

### Operators and Keywords
- `OP`: Comparison operators (`===`, `==`, `~=`, `!=`, `<=`, `>=`, `<`, `>`)
- `BOOLOP`: Boolean operators (`or`, `and`)
- `IN`, `NOT`: Keyword operators

### Identifiers and Values
- `QUOTED_STRING`: Single or double-quoted strings
- `IDENTIFIER`: Alphanumeric identifiers with dots, underscores, hyphens
- `URL`: Space-delimited URLs
- `VARIABLE`: Environment variables for dependency resolution

### Version-Related
- `SPECIFIER`: Version specifiers (combines operator and version patterns)
- `VERSION_PREFIX_TRAIL`: Wildcard version endings (`.*`)
- `VERSION_LOCAL_LABEL_TRAIL`: Local version labels (`+local.1`)

## Usage Example

```python
# Initialize tokenizer
source = "package_name>=1.0,<2.0"
tokenizer = Tokenizer(source, rules=DEFAULT_RULES)

# Parse tokens
if tokenizer.check("IDENTIFIER"):
    package_token = tokenizer.read()
    print(f"Package: {package_token.text}")

if tokenizer.check("SPECIFIER"):
    spec_token = tokenizer.read()
    print(f"Version spec: {spec_token.text}")
```

## Notes and Suggestions

### Best Practices
- Always check for tokens before reading them
- Use `expect()` for required tokens to get clear error messages
- Utilize `enclosing_tokens()` context manager for paired delimiters
- Handle `ParserSyntaxError` exceptions for user-friendly error reporting

### Potential Improvements
- Consider adding line/column tracking for better error reporting
- Add support for custom token rule injection
- Implement token lookahead beyond single token
- Add debugging/logging capabilities for complex parsing scenarios

### Dependencies
- Requires `.specifiers` module for `Specifier` class
- Uses standard library modules: `contextlib`, `re`, `dataclasses`, `typing`

This tokenizer is particularly well-suited for parsing Python package specifications, dependency declarations, and version constraints as commonly found in `requirements.txt`, `setup.py`, and `pyproject.toml` files.