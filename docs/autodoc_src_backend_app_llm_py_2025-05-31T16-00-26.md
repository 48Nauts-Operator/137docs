<!--
This documentation was auto-generated by Claude on 2025-05-31T16-00-26.
Source file: ./src/backend/app/llm.py
-->

# LLM Integration Module Documentation

## Overview

The LLM Integration module provides a comprehensive service for interacting with Large Language Models (LLMs) for document metadata extraction, content analysis, and automated tagging. It supports multiple LLM providers including local Ollama instances, OpenAI, Anthropic, LiteLLM, and custom endpoints.

## Key Features

- **Multi-provider Support**: Compatible with Ollama, OpenAI, Anthropic, LiteLLM, and custom API endpoints
- **Intelligent Metadata Extraction**: Iterative extraction with heuristic enrichment for improved accuracy
- **Document Analysis**: Automated content analysis including sentiment, entities, and key points
- **Auto-tagging**: Intelligent document categorization and tag suggestion
- **Configuration Management**: Database-backed configuration with environment variable fallbacks
- **Connection Testing**: Built-in testing capabilities with detailed debugging
- **Caching**: Configuration caching and response caching for improved performance

## Classes

### LLMService

The main service class for LLM interactions with comprehensive configuration support.

#### Constructor

```python
LLMService(db_session: Optional[AsyncSession] = None)
```

**Parameters:**
- `db_session`: Optional database session for configuration management

#### Core Methods

##### Configuration Methods

###### `get_config() -> Dict[str, Any]`

Retrieves LLM configuration from database with 5-minute caching fallback to environment variables.

**Returns:**
- Dictionary containing complete LLM configuration

**Configuration Keys:**
- `provider`: LLM provider type ('local', 'openai', 'anthropic', 'litellm', 'custom')
- `api_url`: API endpoint URL
- `api_key`: API authentication key
- `model_*`: Model names for different tasks (tagger, enricher, analytics, responder)
- `enabled`: Whether LLM features are enabled
- `auto_tagging`: Enable automatic tag suggestion
- `auto_enrichment`: Enable automatic content enrichment
- `max_retries`: Maximum retry attempts for failed requests
- `retry_delay`: Delay between retry attempts (seconds)
- `batch_size`: Batch processing size
- `concurrent_tasks`: Maximum concurrent processing tasks
- `min_confidence_*`: Minimum confidence thresholds

###### `is_enabled() -> bool`

Checks if LLM features are globally enabled.

**Returns:**
- Boolean indicating if LLM features are active

###### `is_configured() -> bool`

Validates if LLM service has proper configuration for the selected provider.

**Returns:**
- Boolean indicating if configuration is valid and complete

##### Document Processing Methods

###### `extract_metadata(text: str, *, max_attempts: Optional[int] = None, target_score: float = 0.6, task_type: str = 'enricher') -> Dict[str, Any]`

Extracts structured metadata from document text using iterative LLM calls with heuristic enrichment.

**Parameters:**
- `text`: Document text content to analyze
- `max_attempts`: Maximum number of extraction attempts (defaults to config value)
- `target_score`: Target completeness score (0.0-1.0) to achieve
- `task_type`: Type of processing task for model selection

**Returns:**
- Dictionary containing extracted metadata fields

**Extracted Fields:**
- `title`: Document title
- `document_type`: Type classification (invoice, contract, letter, etc.)
- `sender`: Document sender/issuer
- `recipient`: Document recipient
- `document_date`: Document date (ISO format: YYYY-MM-DD)
- `due_date`: Due date if applicable (ISO format)
- `amount`: Total amount including VAT
- `subtotal`: Amount excluding VAT
- `tax_rate`: VAT rate percentage
- `tax_amount`: VAT amount
- `currency`: ISO currency code (CHF, EUR, USD)
- `status`: Document status (paid, unpaid, pending)
- Address fields: `street`, `address2`, `zip`, `town`, `county`, `country`
- Contact fields: `sender_email`, `phone`
- `tags`: Relevant document tags

**Process:**
1. Sends extraction prompt to LLM
2. Parses JSON response
3. Applies heuristic enrichment using regex patterns
4. Scores completeness against target
5. Retries with focused prompts for missing fields if needed

###### `analyze_document(text: str) -> Dict[str, Any]`

Performs comprehensive document content analysis.

**Parameters:**
- `text`: Document text content

**Returns:**
- Dictionary containing analysis results:
  - `summary`: Brief document summary (max 100 words)
  - `key_points`: List of important information
  - `entities`: Named entities (people, organizations, locations)
  - `sentiment`: Overall sentiment (positive, negative, neutral)
  - `action_items`: Required actions or tasks

###### `suggest_tags(text: str) -> List[str]`

Generates relevant tags for document categorization.

**Parameters:**
- `text`: Document text content

**Returns:**
- List of 3-5 suggested tags for the document

##### Testing and Diagnostics

###### `test_connection(provider: str, api_url: Optional[str] = None, api_key: Optional[str] = None) -> Dict[str, Any]`

Tests connectivity to LLM provider with detailed debugging information.

**Parameters:**
- `provider`: Provider type to test
- `api_url`: Optional API URL override
- `api_key`: Optional API key override

**Returns:**
- Dictionary with test results:
  - `status`: 'success' or 'error'
  - `message`: Human-readable result message
  - `available_models`: List of discoverable models
  - `debug_info`: Detailed debugging information

#### Internal Methods

##### LLM Communication

###### `_query_llm(prompt: str, task_type: str = 'enricher') -> str`

Routes LLM queries to appropriate provider endpoint based on configuration.

###### `_query_ollama_direct(prompt: str, api_url: str, model: str) -> str`

Direct communication with Ollama API using native format.

###### `_query_generic_llm_direct(prompt: str, api_url: str, api_key: Optional[str], model: str) -> str