<!--
This documentation was auto-generated by Claude on 2025-06-01T06-17-12.
Source file: ./src/backend/run_tests.py
-->

# LLM Integration Test Runner

## Overview

The `run_llm_tests.py` module provides a test runner specifically designed for executing 137Docs LLM (Large Language Model) integration tests. This utility sets up the necessary testing environment, configures database connections, and executes the test suite with appropriate pytest configurations.

## Features

- Automated test environment setup
- In-memory database configuration for isolated testing
- Comprehensive pytest configuration with async support
- Clear visual feedback and error reporting
- Dependency validation and helpful error messages

## Usage

### Command Line Execution

```bash
python run_llm_tests.py
```

Or make the script executable and run directly:

```bash
chmod +x run_llm_tests.py
./run_llm_tests.py
```

### Programmatic Usage

```python
from run_llm_tests import run_tests

# Execute tests and get return code
exit_code = run_tests()
if exit_code == 0:
    print("Tests completed successfully")
else:
    print(f"Tests failed with code: {exit_code}")
```

## Environment Configuration

The test runner automatically configures the following environment variables:

| Variable | Value | Purpose |
|----------|-------|---------|
| `TESTING` | `true` | Indicates the application is running in test mode |
| `DATABASE_URL` | `sqlite+aiosqlite:///:memory:` | Configures an in-memory SQLite database for testing |

## Pytest Configuration

The runner executes pytest with the following parameters:

- **Target**: `tests/test_llm_simple.py` - Specific test file for LLM functionality
- **Verbosity**: `-v` - Verbose output showing individual test results
- **Traceback**: `--tb=short` - Concise error tracebacks
- **Async Mode**: `--asyncio-mode=auto` - Automatic async test detection
- **Output**: `-s` - Preserve print statements in test output

## Functions

### `run_tests()`

Executes the LLM integration test suite with proper environment setup.

**Returns:**
- `int`: Exit code (0 for success, non-zero for failure)

**Behavior:**
1. Sets up testing environment variables
2. Constructs pytest command with appropriate flags
3. Executes the test suite using subprocess
4. Provides visual feedback with emojis and status messages
5. Returns appropriate exit codes for integration with CI/CD systems

**Error Handling:**
- **subprocess.CalledProcessError**: Captures test failures and returns the pytest exit code
- **FileNotFoundError**: Detects missing pytest installation and provides installation instructions

## Dependencies

### Required Dependencies

- **Python 3.6+**: Required for f-string support and subprocess features
- **pytest**: Test framework for executing the test suite
- **pytest-asyncio**: Plugin for testing async functions
- **pytest-mock**: Mocking framework for tests (recommended)

### Installation

```bash
pip install pytest pytest-asyncio pytest-mock
```

## Exit Codes

| Code | Meaning |
|------|---------|
| 0 | All tests passed successfully |
| 1 | pytest not found or installation error |
| >1 | Test failures (returned from pytest) |

## Example Output

### Successful Test Run
```
üß™ Running 137Docs LLM Integration Tests
==================================================
========================= test session starts =========================
collected 5 items

tests/test_llm_simple.py::test_llm_connection PASSED
tests/test_llm_simple.py::test_llm_response PASSED
tests/test_llm_simple.py::test_llm_error_handling PASSED
tests/test_llm_simple.py::test_llm_async_operations PASSED
tests/test_llm_simple.py::test_llm_integration PASSED

========================= 5 passed in 2.34s =========================

‚úÖ All LLM integration tests passed!
```

### Failed Test Run
```
üß™ Running 137Docs LLM Integration Tests
==================================================
========================= FAILURES =========================
_________________ test_llm_connection _________________
[test failure details]

‚ùå Tests failed with exit code 1
```

## Integration with CI/CD

The test runner is designed to work seamlessly with continuous integration systems:

```yaml
# Example GitHub Actions workflow
- name: Run LLM Integration Tests
  run: python run_llm_tests.py
  
- name: Check test results
  if: ${{ failure() }}
  run: echo "LLM integration tests failed"
```

## Best Practices

1. **Isolation**: Each test run uses a fresh in-memory database
2. **Environment**: Tests run in a controlled environment separate from development/production
3. **Feedback**: Clear visual indicators help identify test status quickly
4. **Error Handling**: Comprehensive error messages guide users toward solutions
5. **Exit Codes**: Proper exit codes enable integration with automated systems