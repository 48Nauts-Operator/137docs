<!-- Auto-generated by Claude on 2025-06-01 10:12 -->

# LLM Configuration Database Migration

## Overview

This file is an **Alembic database migration script** that creates the `llm_config` table for local-first AI integration. It's part of a database schema evolution system that manages Large Language Model (LLM) configuration settings.

## Purpose

The migration establishes a centralized configuration table to store:
- LLM provider settings (local/cloud)
- Model preferences for different AI tasks
- Processing and performance parameters
- Reliability and backup configurations
- Confidence thresholds for AI operations

## Migration Details

| Property | Value |
|----------|-------|
| **Revision ID** | `20250525_llm_config` |
| **Previous Migration** | `20250525_entities` |
| **Created** | 2025-05-25 |

## Database Schema

### Table: `llm_config`

The migration creates a comprehensive configuration table with the following column groups:

#### Provider Configuration
```sql
provider         VARCHAR(50)  DEFAULT 'local'    -- LLM provider type
api_key          VARCHAR(255) NULL               -- API authentication key
api_url          VARCHAR(255) NULL               -- Custom API endpoint
```

#### Model Assignments
```sql
model_tagger     VARCHAR(100) DEFAULT 'phi3'     -- Model for tagging tasks
model_enricher   VARCHAR(100) DEFAULT 'llama3'   -- Model for content enrichment
model_analytics  VARCHAR(100) DEFAULT 'llama3'   -- Model for analytics
model_responder  VARCHAR(100) NULL               -- Model for responses
```

#### Processing Settings
```sql
enabled             BOOLEAN DEFAULT false    -- Master AI enable/disable
auto_tagging        BOOLEAN DEFAULT true     -- Automatic tagging
auto_enrichment     BOOLEAN DEFAULT true     -- Automatic enrichment
external_enrichment BOOLEAN DEFAULT false    -- Use external data sources
```

#### Reliability Features
```sql
max_retries      INTEGER DEFAULT 3          -- Maximum retry attempts
retry_delay      INTEGER DEFAULT 300        -- Delay between retries (seconds)
backup_provider  VARCHAR(50) NULL           -- Fallback provider
backup_model     VARCHAR(100) NULL          -- Fallback model
```

#### Performance Tuning
```sql
batch_size       INTEGER DEFAULT 5          -- Items per batch
concurrent_tasks INTEGER DEFAULT 2          -- Parallel processing limit
cache_responses  BOOLEAN DEFAULT true       -- Enable response caching
```

#### Quality Controls
```sql
min_confidence_tagging FLOAT DEFAULT 0.7    -- Minimum tagging confidence
min_confidence_entity  FLOAT DEFAULT 0.8    -- Minimum entity confidence
```

## Key Functions

### `upgrade()`
- **Purpose**: Applies the migration forward
- **Actions**:
  - Creates the `llm_config` table with all columns and defaults
  - Establishes primary key index on `id` column
- **Default Values**: Sets sensible defaults for local-first AI setup

### `downgrade()`
- **Purpose**: Rolls back the migration
- **Actions**:
  - Drops the index on `llm_config.id`
  - Removes the entire `llm_config` table

## Usage Notes

### Default Configuration
The migration sets up a **local-first** configuration by default:
- Provider: `local`
- Tagging model: `phi3` (lightweight)
- Enrichment/Analytics: `llama3` (more capable)
- Conservative confidence thresholds
- Moderate performance settings

### Configuration Strategy
```python
# Example of how this table might be used:
config = {
    'provider': 'local',           # or 'openai', 'anthropic', etc.
    'model_tagger': 'phi3',        # Fast, lightweight model
    'model_enricher': 'llama3',    # More capable model
    'batch_size': 5,               # Process 5 items at once
    'min_confidence_tagging': 0.7   # 70% confidence threshold
}
```

## Important Considerations

### Performance
- **Batch Processing**: Default batch size of 5 balances throughput and memory usage
- **Concurrency**: Limited to 2 concurrent tasks to prevent resource exhaustion
- **Caching**: Enabled by default to improve response times

### Reliability
- **Retry Logic**: 3 attempts with 5-minute delays
- **Backup Strategy**: Optional fallback provider/model configuration
- **Confidence Filtering**: Quality gates to prevent low-confidence results

### Flexibility
- **Multi-Model Support**: Different models for different task types
- **Provider Agnostic**: Supports local and cloud-based LLM providers
- **Feature Toggles**: Granular control over AI processing features

## Recommendations

1. **Monitor Performance**: Adjust `batch_size` and `concurrent_tasks` based on system resources
2. **Tune Confidence**: Start with default thresholds and adjust based on result quality
3. **Backup Planning**: Configure backup provider for production reliability
4. **Security**: Ensure `api_key` values are properly encrypted at the application level