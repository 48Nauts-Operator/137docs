<!-- Auto-generated by Claude on 2025-06-01 10:12 -->

# LLM Integration Module Documentation

## Overview

This module provides LLM (Large Language Model) integration for metadata extraction and document analysis. It supports multiple LLM providers including local Ollama instances and OpenAI-compatible APIs, with configurable settings and robust error handling.

## Purpose

The module serves to:
- Extract structured metadata from document text using LLM analysis
- Analyze document content for insights and categorization  
- Suggest relevant tags for documents
- Provide connection testing and model discovery for various LLM providers
- Enhance LLM results with regex-based heuristic enrichment

## Main Classes

### `LLMService`

The primary class for interacting with LLM APIs with database configuration support.

#### Constructor
```python
def __init__(self, db_session: Optional[AsyncSession] = None)
```
- **Parameters**: 
  - `db_session`: Optional database session for loading configuration

#### Key Methods

##### Configuration Management
- **`get_config()`**: Retrieves LLM configuration from database with 5-minute caching, falls back to environment variables
- **`is_enabled()`**: Checks if LLM features are enabled
- **`is_configured()`**: Validates if LLM service has proper configuration

##### Document Processing
- **`extract_metadata(text, *, max_attempts=None, target_score=0.6, task_type='enricher')`**: 
  - Extracts structured metadata from document text
  - Performs iterative refinement until target completeness score is reached
  - Includes regex-based heuristic enrichment
  
- **`analyze_document(text)`**: Performs comprehensive document analysis returning summary, key points, entities, sentiment, and action items

- **`suggest_tags(text)`**: Generates relevant tags for document categorization

##### Connection Testing
- **`test_connection(provider, api_url=None, api_key=None)`**: Tests connection to LLM provider with detailed debugging information

## Configuration

The service supports multiple configuration sources:

### Database Configuration
Configuration is cached for 5 minutes and includes:
- Provider settings (local, openai, anthropic, litellm, custom)
- API URLs and keys
- Model assignments for different tasks
- Feature flags and behavior settings

### Environment Variable Fallback
```python
fallback_config = {
    'provider': 'local',
    'api_url': os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434"),
    'api_key': settings.LLM_API_KEY,
    'model_tagger': settings.LLM_MODEL,
    'model_enricher': settings.LLM_MODEL,
    'model_analytics': settings.LLM_MODEL,
    'model_responder': settings.LLM_MODEL,
    'enabled': True,
    'auto_tagging': True,
    'auto_enrichment': True,
    # ... additional settings
}
```

## Supported Providers

- **Local (Ollama)**: Self-hosted Ollama instances
- **OpenAI**: Official OpenAI API
- **Anthropic**: Claude API
- **LiteLLM**: LiteLLM proxy server
- **Custom**: Generic OpenAI-compatible APIs

## Metadata Extraction Features

### Required Fields
The system tracks completion of required fields for scoring:
```python
_REQUIRED_FIELDS = {
    "title": True,
    "document_type": True, 
    "sender": True,
    "amount": True,
    "currency": True,
    # Optional fields...
}
```

### Heuristic Enrichment
Post-processes LLM results with regex patterns for:
- **Email addresses**: `[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}`
- **Phone numbers**: `\+?\d[\d\s\-]{7,}`
- **Currency amounts**: `CHF|EUR|USD|GBP` with numeric patterns
- **Address blocks**: Swiss/EU format address parsing
- **VAT information**: Swiss UID format and VAT calculations

## Usage Examples

### Basic Metadata Extraction
```python
llm_service = LLMService(db_session)
metadata = await llm_service.extract_metadata(
    document_text,
    target_score=0.8,
    max_attempts=3
)
```

### Document Analysis
```python
analysis = await llm_service.analyze_document(document_text)
# Returns: summary, key_points, entities, sentiment, action_items
```

### Connection Testing
```python
result = await llm_service.test_connection(
    provider='local',
    api_url='http://localhost:11434'
)
```

## Error Handling

The module includes comprehensive error handling:
- Network timeouts and connection errors
- JSON parsing failures with json5 fallback
- Provider-specific error responses
- Configuration validation
- Graceful degradation when LLM features are disabled

## Notes and Suggestions

### Performance Considerations
- Configuration caching reduces database queries
- Concurrent task limits prevent API overwhelming
- Retry mechanisms with configurable delays
- Text truncation for large documents (3000 chars)

### Date Format Requirements
⚠️ **Critical**: All dates must be in ISO format (YYYY-MM-DD). The system explicitly converts European (DD.MM.YYYY) and American (MM/DD/YYYY) formats.

### Backward Compatibility
- `LLMProcessor` class provided as alias for existing code
- Legacy method support maintained

### Recommended Settings
- Set appropriate `target_score` based on use case (0.6-0.8)
- Configure `max_retries` and `retry_delay` for your API limits
- Enable `cache_responses` for repeated processing
- Set confidence thresholds for tagging and entity extraction

### Model Selection
Different models can be configured for specific tasks:
- `model_tagger`: Tag suggestion
- `model_enricher`: Metadata extraction  
- `model_analytics`: Document analysis
- `model_responder`: General responses

This allows optimization of model choice based on task requirements and cost considerations.